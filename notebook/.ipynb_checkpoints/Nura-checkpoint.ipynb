{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6baecba5-577b-479f-b5b3-12282975aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§  Step 1: Setup & Imports\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32a4d71a-04b9-4924-8997-20a356000f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instruction', 'input', 'output']\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"ShenLab/MentalChat16K\", split='train')\n",
    "print(ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe9d5624-3604-4ad8-b152-65b467ec31a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'iterrows'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pairs, tags\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# df is already loaded from Hugging Face or JSON\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m pairs, tags \u001b[38;5;241m=\u001b[39m \u001b[43mload_conversation_data_from_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pairs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m conversation pairs (after skipping first 100).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 6\u001b[0m, in \u001b[0;36mload_conversation_data_from_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      3\u001b[0m tags \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Skip first 100 rows using .iloc\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m():\n\u001b[1;32m      7\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      8\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'iterrows'"
     ]
    }
   ],
   "source": [
    "def load_conversation_data_from_hf(ds):\n",
    "    pairs = []\n",
    "    tags = []\n",
    "\n",
    "    # Skip first 100 rows\n",
    "    for row in ds:\n",
    "        # If \"input\" is empty, use \"instruction\" as context\n",
    "        context = str(row[\"input\"]).strip() if str(row[\"input\"]).strip() else str(row[\"instruction\"]).strip()\n",
    "        response = str(row[\"output\"]).strip()\n",
    "        pairs.append((context, response))\n",
    "        tags.append(\"mental_health\")  # generic tag\n",
    "\n",
    "    return pairs, tags\n",
    "\n",
    "# df is already loaded from Hugging Face or JSON\n",
    "pairs, tags = load_conversation_data_from_df(ds)\n",
    "print(f\"Loaded {len(pairs)} conversation pairs (after skipping first 100).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a589d41-f156-4977-bfbd-02fd382e82ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Conversation embeddings stored in Chroma.\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§¬ Step 3: Embed Patterns & Store in Chroma DB\n",
    "\n",
    "texts = [p[0] for p in pairs]\n",
    "metadatas = [{\"response\": p[1], \"tag\": tag} for p, tag in zip(pairs, tags)]\n",
    "\n",
    "# Create embeddings and store in Chroma\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "chroma_db = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embedding_model,\n",
    "    metadatas=metadatas,\n",
    "    collection_name=\"pandora_conversations\",  # Changed collection name\n",
    "    persist_directory=\"./chroma_langchain_db\"\n",
    ")\n",
    "print(\"âœ… Conversation embeddings stored in Chroma.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5635ed90-5d93-4df2-89a6-e79ad2d2eee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Crying is clearing. It is a release of energy and toxins. Crying is a good thing and a great way to let go and move forward!'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ðŸ” Step 4: Test Semantic Query\n",
    "def semantic_response(query, db):\n",
    "    results = db.similarity_search(query, k=1)\n",
    "    if results:\n",
    "        return results[0].metadata['response']\n",
    "    return \"I'm not sure how to respond to that.\"\n",
    "\n",
    "semantic_response(\"I'm very upset.\", chroma_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18ae6aa3-48b0-4636-a007-8463f972079d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Intent classifier trained.\n"
     ]
    }
   ],
   "source": [
    "# ðŸ¤– Step 5: Train Intent Classifier\n",
    "X = [p[0] for p in pairs]\n",
    "y = tags\n",
    "\n",
    "classifier = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "classifier.fit(X, y)\n",
    "print(\"âœ… Intent classifier trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e68ff49-2355-47ab-b654-6237b89a8611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm listening. Tell me more.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ðŸ“¦ Step 6: Predict Tag & Response\n",
    "def predict_intent(query, model, intent_data):\n",
    "    tag = model.predict([query])[0]\n",
    "    \n",
    "    for intent in intent_data[\"intents\"]:\n",
    "        if intent[\"tag\"] == tag:\n",
    "            return random.choice(intent[\"responses\"])\n",
    "    \n",
    "    return \"I'm not sure how to respond.\"\n",
    "\n",
    "predict_intent(\"My friends bully me\", classifier, raw_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87f652f7-2656-4534-a634-ac7e2597ac76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry to hear that. Just know that I'm here for you. Talking about it might help. Why do you think you don't have any friends?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hybrid_response(query: str, \n",
    "                    classifier: Pipeline, \n",
    "                    intent_data: dict, \n",
    "                    chroma_db, \n",
    "                    threshold: float = 0.4) -> str:\n",
    "    \"\"\"\n",
    "    Combines intent classification and semantic similarity for response generation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Get predicted intent and confidence\n",
    "    intent_proba = classifier.predict_proba([query])[0]\n",
    "    max_proba_index = intent_proba.argmax()\n",
    "    predicted_tag = classifier.classes_[max_proba_index]\n",
    "    confidence = intent_proba[max_proba_index]\n",
    "\n",
    "    # Step 2: High confidence â€” use intent-based response\n",
    "    if confidence >= threshold:\n",
    "        for intent in intent_data[\"intents\"]:\n",
    "            if intent[\"tag\"] == predicted_tag:\n",
    "                return random.choice(intent[\"responses\"])\n",
    "\n",
    "    # Step 3: Low confidence â€” use semantic fallback\n",
    "    results = chroma_db.similarity_search(query, k=1)\n",
    "    if results:\n",
    "        return results[0].metadata['response']\n",
    "\n",
    "    return \"I'm here for you, but Iâ€™m not sure how to help with that.\"\n",
    "\n",
    "hybrid_response(\"I don't have any friends at school so going there makes me scared.\", classifier, raw_data, chroma_db)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nurawell-env",
   "language": "python",
   "name": "nurawell-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
